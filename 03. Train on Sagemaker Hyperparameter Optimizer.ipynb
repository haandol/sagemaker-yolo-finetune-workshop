{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO v3 Finetuning on AWS\n",
    "\n",
    "This series of notebooks demonstrates how to finetune pretrained YOLO v3 (aka YOLO3) using MXNet on AWS.\n",
    "\n",
    "**This notebook** walks through using the [SageMaker Hyperparameter Tuning Job](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-how-it-works.html) tool to finding optmized hypterparameter and finetune the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Follow-on** the content of the notebooks shows:\n",
    "\n",
    "* How to use MXNet YOLO3 pretrained model\n",
    "* How to use Deep SORT with MXNet YOLO3\n",
    "* How to create Ground-Truth dataset from images the model mis-detected\n",
    "* How to finetune the model using the created dataset\n",
    "* Load your finetuned model and Deploy Sagemaker-Endpoint with it using CPU instance.\n",
    "* Load your finetuned model and Deploy Sagemaker-Endpoint with it using GPU instance.\n",
    "\n",
    "## Pre-requisites\n",
    "\n",
    "This notebook is designed to be run in Amazon SageMaker. To run it (and understand what's going on), you'll need:\n",
    "\n",
    "* Basic familiarity with Python, [MXNet](https://mxnet.apache.org/), [AWS S3](https://docs.aws.amazon.com/s3/index.html), [Amazon SageMaker](https://aws.amazon.com/sagemaker/)\n",
    "* To create an **S3 bucket** in the same region, and ensure the SageMaker notebook's role has access to this bucket.\n",
    "* Sufficient [SageMaker quota limits](https://docs.aws.amazon.com/general/latest/gr/aws_service_limits.html#limits_sagemaker) set on your account to run GPU-accelerated spot training jobs.\n",
    "\n",
    "## Cost and runtime\n",
    "\n",
    "Depending on your configuration, this demo may consume resources outside of the free tier but should not generally be expensive because we'll be training on a small number of images. You might wish to review the following for your region:\n",
    "\n",
    "* [Amazon SageMaker pricing](https://aws.amazon.com/sagemaker/pricing/)\n",
    "\n",
    "The standard `ml.t2.medium` instance should be sufficient to run the notebooks.\n",
    "\n",
    "We will use GPU-accelerated instance types for training and hyperparameter optimization, and use spot instances where appropriate to optimize these costs.\n",
    "\n",
    "As noted in the step-by-step guidance, you should take particular care to delete any created SageMaker real-time prediction endpoints when finishing the demo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0: Dependencies and configuration\n",
    "\n",
    "As usual we'll start by loading libraries, defining configuration, and connecting to the AWS SDKs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dongkyl/.pyenv/versions/3.7.4/lib/python3.7/site-packages/pandas/compat/__init__.py:85: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "# Built-Ins:\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from datetime import date, datetime\n",
    "from glob import glob\n",
    "from pprint import pprint\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3\n",
    "import imageio\n",
    "import sagemaker\n",
    "import numpy as np\n",
    "from sagemaker.mxnet import MXNet\n",
    "from botocore.exceptions import ClientError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restore stored variables\n",
    "%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "s3 = session.resource('s3')\n",
    "bucket = s3.Bucket(BUCKET_NAME)\n",
    "smclient = session.client('sagemaker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "iam = boto3.client('iam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker-ap-northeast-2-929831892372\n"
     ]
    }
   ],
   "source": [
    "print(bucket.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Recap output.manifest\n",
    "\n",
    "In last notebook, we made the *output.manifest* that is containing annotation infromation along with image location. And here is the content of the file.\n",
    "\n",
    "content is dictionary having 2 essential keys, *labels* and *source-ref*. \n",
    "- **labels** - contains information of bounding boxes in the value under key *annotations*.  *class_id* is always *0* because we have only one class *person* in the dataset.\n",
    "- **source-ref** - same value as in *input.manifest* file\n",
    "\n",
    "For introduction to model training and deployment, see [**Train a Model with Amazon SageMaker**](http://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-training.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_manifest_path = f'annotations/{job_name}/manifests/output/output.manifest'\n",
    "output_manifest_obj = bucket.Object(output_manifest_path)\n",
    "dataset = output_manifest_obj.get()['Body'].read().decode('utf-8').split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'labels': {'annotations': [{'class_id': 0,\n",
      "                             'height': 230,\n",
      "                             'left': 90,\n",
      "                             'top': 45,\n",
      "                             'width': 182},\n",
      "                            {'class_id': 0,\n",
      "                             'height': 231,\n",
      "                             'left': 150,\n",
      "                             'top': 0,\n",
      "                             'width': 174}],\n",
      "            'image_size': [{'depth': 3, 'height': 320, 'width': 427}]},\n",
      " 'labels-metadata': {'class-map': {'0': 'Person'},\n",
      "                     'creation-date': '2020-03-14T12:51:04.692288',\n",
      "                     'human-annotated': 'yes',\n",
      "                     'job-name': 'labeling-job/yolo-job-0',\n",
      "                     'objects': [{'confidence': 0.09}, {'confidence': 0.09}],\n",
      "                     'type': 'groundtruth/object-detection'},\n",
      " 'source-ref': 's3://sagemaker-ap-northeast-2-929831892372/yolo-workshop-batch/images/235.jpg'}\n"
     ]
    }
   ],
   "source": [
    "pprint(json.loads(dataset[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Split dataset into Train and Test datasets\n",
    "\n",
    "Split dataset into train and test datasets is common procedure in Machine Learning(ML).\n",
    "\n",
    "There are several methods to do that, and we are going to use the simplest one in here. We are going to shuffle entire dataset and split with ratio of 9:1 for train and test respectively.\n",
    "\n",
    "After split, you will get 2 files, *train.manifest* and *test.manifest* in the path that *output.manifest* is located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'n_samples_train' (int)\n",
      "Stored 'n_samples_test' (int)\n",
      "Training manifest uploaded to:\n",
      "s3://sagemaker-ap-northeast-2-929831892372/annotations/yolo-job-0/manifests/output/train.manifest\n",
      "Test manifest uploaded to:\n",
      "s3://sagemaker-ap-northeast-2-929831892372/annotations/yolo-job-0/manifests/output/test.manifest\n"
     ]
    }
   ],
   "source": [
    "TRAIN_RATIO = 0.9\n",
    "\n",
    "# shuffle dataset\n",
    "np.random.shuffle(dataset)\n",
    "\n",
    "n_samples_total = len(dataset)\n",
    "train_test_split_index = round(n_samples_total*TRAIN_RATIO)\n",
    "\n",
    "# split datasets\n",
    "train_dataset = dataset[:train_test_split_index]\n",
    "test_dataset = dataset[train_test_split_index:]\n",
    "\n",
    "n_samples_train = len(train_dataset)\n",
    "%store n_samples_train\n",
    "n_samples_test = len(test_dataset)\n",
    "%store n_samples_test\n",
    "\n",
    "# store manifests into localhost\n",
    "with open(f'train.manifest', 'w') as f:\n",
    "    for line in train_dataset:\n",
    "        if not line:\n",
    "            continue\n",
    "        f.write(str(line))\n",
    "        f.write(\"\\n\")\n",
    "    \n",
    "with open(f'test.manifest', 'w') as f:\n",
    "    for line in test_dataset:\n",
    "        if not line:\n",
    "            continue\n",
    "        f.write(str(line))\n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "# store train/test manifests to s3 bucket where output.manifest is located.\n",
    "manifest_path = output_manifest_path.rsplit('/', 1)[0]\n",
    "bucket.upload_file('train.manifest', f'{manifest_path}/train.manifest')\n",
    "print('Training manifest uploaded to:\\n' + f's3://{bucket.name}/{manifest_path}/train.manifest')\n",
    "bucket.upload_file('test.manifest', f'{manifest_path}/test.manifest')\n",
    "print('Test manifest uploaded to:\\n' + f\"s3://{bucket.name}/{manifest_path}/test.manifest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create Hyperparameter Tuning Job\n",
    "\n",
    "Now, you are ready to finetune MXNet YOLO model with *train.manifest* and *test.manifest* datasets.\n",
    "\n",
    "Of course, you create hyperparameter tuning job on AWS Console but there is much easier way to do the same job on sagemaker notebook.\n",
    "\n",
    "Sagemaker provide *sagemaker.mxnet.MXNet* estimator to train model. With this class you can train or make hyperparameter tuning job for your own model.\n",
    "\n",
    "First of all, you should define metric for estimator. The estimator's goal is mininize or maximize the metric you gave to it.\n",
    "\n",
    "In this chapter we are going to use *Loss* as a metric which means the goal of the estimator is going to be minize it as much as it can. \n",
    "\n",
    "The estimator container will automatically capture it's *stdout* and find the *Regex* pattern you difined and make it as metric to minize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_definitions = [\n",
    "    { 'Name': 'TrainLoss', 'Regex': 'Train Loss: (.*?) ;' },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have execution role for sagemaker just put it to the *role_name* on below cell.\n",
    "\n",
    "Let's make IAM role on [**AWS Console**]() with *AmazonSagemakerFullAccess* Policy like below screen. \n",
    "\n",
    "<img src=\"Assets/ExecutionRole.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'role_name' (str)\n"
     ]
    }
   ],
   "source": [
    "# replace role_name with yours\n",
    "role_name = 'AmazonSageMaker-ExecutionRole-20200129T183159'\n",
    "%store role_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = iam.get_role(RoleName=role_name)['Role']['Arn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make estiamtor. Estimator handles end-to-end Amazon SageMaker training and deployment tasks.\n",
    "\n",
    "You can run your training job on *Spot Instance* and we are going to do that, because using spot instance is the most cost efficient way to run your job on AWS.\n",
    "\n",
    "The estimator we are making, uses *4 of ml.p3.8xlarge Spot instances* for training so that 4 Hyperparameter tuning job is able to run cucurrently.\n",
    "\n",
    "Let me explain some important parameters before run the code,\n",
    "\n",
    "* entry_point - python script that includes train/finetune logics.\n",
    "* source_dir - local folder location that `entry_point` is placed.\n",
    "* frame_work_version - MXNet framework version\n",
    "* input_mode - 'File' or 'Pipe'. entry_point should be implemented considering input_mode.\n",
    "* train_use_spot_instances - True if you want to use spot-instance for running training jobs.\n",
    "* output_path - s3 bucket path that models and checkpoints will be stored.\n",
    "* hyperparameters - default hyperparameters. most of the values will be overriden by hyperparameter tuning job. (look into *hyperparameter_ranges* variable below cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'model_output_path' (str)\n"
     ]
    }
   ],
   "source": [
    "model_output_path = f's3://{BUCKET_NAME}/{MODELS_PREFIX}'\n",
    "%store model_output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = MXNet(\n",
    "    role=role,\n",
    "    entry_point='yolo_finetune.py',\n",
    "    source_dir='src',\n",
    "    framework_version='1.4.1',\n",
    "    py_version='py3',\n",
    "    input_mode='File',\n",
    "    train_volume_size=n_samples_train,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.p3.8xlarge',\n",
    "    train_max_run=5*60*60,\n",
    "    train_use_spot_instances=True,\n",
    "    train_max_wait=5*60*60,\n",
    "    metric_definitions=metric_definitions,\n",
    "    base_job_name='yolo-finetune-0',\n",
    "    output_path=model_output_path,\n",
    "    hyperparameters={\n",
    "        'epochs': 30,\n",
    "        'num-workers': 4,\n",
    "        'batch-size': 8,\n",
    "        'num-gpus': 4,\n",
    "        'data-shape': 320,\n",
    "        'lr': 0.000361,\n",
    "        'momentum': 0.299848,\n",
    "        'wd': 0.986724,\n",
    "        'optimizer': 'sgd',\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Prepare channel\n",
    "\n",
    "HyperParameter tuning job requires data channel for fetch data from s3.\n",
    "\n",
    "Estimator on *File* mode, *image_channel* must be provided to the tuner because Sagemaker training container copies all train/test images on creating container instance using *image_channel*.\n",
    "\n",
    "We are using *File* mode because our dataset is small enough but if you are planning to deal with very large dataset consider *Pipe* mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass only essential keys\n",
    "attribute_names = ['source-ref', 'labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_channel = sagemaker.session.s3_input(\n",
    "    f's3://{BUCKET_NAME}/{manifest_path}/train.manifest',\n",
    "    distribution='FullyReplicated',\n",
    "    s3_data_type=\"S3Prefix\",\n",
    "    attribute_names=attribute_names\n",
    ")\n",
    "                                        \n",
    "test_channel = sagemaker.session.s3_input(\n",
    "    f's3://{BUCKET_NAME}/{manifest_path}/test.manifest',\n",
    "    distribution='FullyReplicated',\n",
    "    s3_data_type='S3Prefix',\n",
    "    attribute_names=attribute_names\n",
    ")\n",
    "\n",
    "image_channel = sagemaker.session.s3_input(\n",
    "    f's3://{BUCKET_NAME}/{BATCH_NAME}/{IMAGE_PREFIX}',\n",
    "    s3_data_type=\"S3Prefix\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Finetune Model using Hyperparameter tuning job\n",
    "\n",
    "[How Hyperparameter Tuning Works](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-how-it-works.html) says dailed information about hyperparmeter tuning job.\n",
    "\n",
    "Simply put it, hyperparameter tuning job test all of the *likely* parameters in the given range, and find best combination of the parameters for the model with given dataset.\n",
    "\n",
    "In this manner, you should provide ranges of the hyperparameters where the best parameters lie on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_ranges = {\n",
    "    'lr': sagemaker.tuner.ContinuousParameter(0.0001, 0.1),\n",
    "    'momentum': sagemaker.tuner.ContinuousParameter(0.0, 0.99),\n",
    "    'wd': sagemaker.tuner.ContinuousParameter(0.0, 0.99),\n",
    "    'optimizer': sagemaker.tuner.CategoricalParameter(['sgd', 'adam', 'rmsprop', 'adadelta'])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the all together, such as estimator, metric(or loss), hyperparameter ranges, we are going to run Hyperparameter Tuning Job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'max_jobs' (int)\n",
      "Stored 'train_start_time' (datetime)\n"
     ]
    }
   ],
   "source": [
    "max_jobs = 12\n",
    "%store max_jobs\n",
    "\n",
    "tuner = sagemaker.tuner.HyperparameterTuner(\n",
    "    estimator,\n",
    "    'TrainLoss',\n",
    "    objective_type='Minimize',\n",
    "    metric_definitions=metric_definitions,\n",
    "    hyperparameter_ranges=hyperparameter_ranges,\n",
    "    base_tuning_job_name='yolo-htj-batch-0',\n",
    "    max_jobs=max_jobs,\n",
    "    max_parallel_jobs=3\n",
    ")\n",
    "\n",
    "train_start_time = datetime.now()\n",
    "%store train_start_time\n",
    "\n",
    "tuner.fit(\n",
    "    {\n",
    "        \"train\": train_channel,\n",
    "        \"test\": test_channel,\n",
    "        \"images\": image_channel\n",
    "    },\n",
    "    include_cls_metadata=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you call *fit()* method, you can check the progress on [AWS Console](https://console.aws.amazon.com).\n",
    "\n",
    "<img src=\"Assets/TrainingJobStatus.png\" />\n",
    "\n",
    "and, of course, you can check progress out on the notebook using Sagemaker Python SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'training_job_name' (str)\n"
     ]
    }
   ],
   "source": [
    "# wait for the first job is going to be ready to trating\n",
    "time.sleep(10)\n",
    "\n",
    "training_jobs = smclient.list_training_jobs(NameContains=tuner.base_tuning_job_name, StatusEquals='InProgress')\n",
    "training_job_summaries = training_jobs['TrainingJobSummaries']\n",
    "training_job_name = training_job_summaries[0]['TrainingJobName'].rsplit('-', 2)[0]\n",
    "%store training_job_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lr</th>\n",
       "      <th>momentum</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>wd</th>\n",
       "      <th>TrainingJobName</th>\n",
       "      <th>TrainingJobStatus</th>\n",
       "      <th>FinalObjectiveValue</th>\n",
       "      <th>TrainingStartTime</th>\n",
       "      <th>TrainingEndTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.007768</td>\n",
       "      <td>0.940174</td>\n",
       "      <td>\"rmsprop\"</td>\n",
       "      <td>0.820159</td>\n",
       "      <td>yolo-htj-batch-0-200320-1546-003-ef269a51</td>\n",
       "      <td>InProgress</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000227</td>\n",
       "      <td>0.556847</td>\n",
       "      <td>\"adam\"</td>\n",
       "      <td>0.912397</td>\n",
       "      <td>yolo-htj-batch-0-200320-1546-002-6c81679f</td>\n",
       "      <td>InProgress</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000613</td>\n",
       "      <td>0.046775</td>\n",
       "      <td>\"sgd\"</td>\n",
       "      <td>0.975574</td>\n",
       "      <td>yolo-htj-batch-0-200320-1546-001-440c6ec3</td>\n",
       "      <td>InProgress</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         lr  momentum  optimizer        wd  \\\n",
       "0  0.007768  0.940174  \"rmsprop\"  0.820159   \n",
       "1  0.000227  0.556847     \"adam\"  0.912397   \n",
       "2  0.000613  0.046775      \"sgd\"  0.975574   \n",
       "\n",
       "                             TrainingJobName TrainingJobStatus  \\\n",
       "0  yolo-htj-batch-0-200320-1546-003-ef269a51        InProgress   \n",
       "1  yolo-htj-batch-0-200320-1546-002-6c81679f        InProgress   \n",
       "2  yolo-htj-batch-0-200320-1546-001-440c6ec3        InProgress   \n",
       "\n",
       "  FinalObjectiveValue TrainingStartTime TrainingEndTime  \n",
       "0                None              None            None  \n",
       "1                None              None            None  \n",
       "2                None              None            None  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analytics = sagemaker.HyperparameterTuningJobAnalytics(training_job_name)\n",
    "analytics.dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to wait for all training job is completed, it will take few minues.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_n_completed_jobs(base_job_name, max_jobs, start_time):\n",
    "    counts = 0\n",
    "    date_suffix = date.today().strftime('%y%m%d')\n",
    "    job_name = f'{base_job_name}-{date_suffix}'\n",
    "    for status in ['Completed', 'Failed', 'Stopped']:\n",
    "        counts += len(smclient.list_training_jobs(\n",
    "            NameContains=job_name,\n",
    "            StatusEquals='Completed',\n",
    "            MaxResults=max_jobs,\n",
    "            CreationTimeAfter=start_time,\n",
    "        )['TrainingJobSummaries'])\n",
    "    return counts\n",
    " \n",
    "\n",
    "def wait_for_training(base_job_name, max_jobs, start_time):\n",
    "    completed_jobs = 0\n",
    "    while completed_jobs < max_jobs:\n",
    "        print(f'[{base_job_name}] {completed_jobs}/{max_jobs} of training jobs are completed...')\n",
    "        completed_jobs = fetch_n_completed_jobs(base_job_name, max_jobs, start_time)\n",
    "        time.sleep(30)\n",
    "    print(f'All({max_jobs}) training jobs are completed!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[yolo-htj-batch-0] 0/12 of training jobs are completed...\n",
      "[yolo-htj-batch-0] 0/12 of training jobs are completed...\n"
     ]
    }
   ],
   "source": [
    "wait_for_training(tuner.base_tuning_job_name, max_jobs, train_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
